{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Thymio escape\n",
    "\n",
    "For the fall semester of 2025, the 24th team below joined their forces to save Thymio, a small squared-shaped robot lost in its world. To escape, he needs to join the portal as soon as possible and avoid any obstacles in his way.\n",
    "\n",
    "- 341237 – Louis Grange\n",
    "- 362819 – Alessandro De Zen\n",
    "- 328691 – Verter Stoilov\n",
    "- 286962 – Charlotte Délia Alice Maria Vadori\n",
    "\n",
    "To achieve this hard and dangerous mission, Louis helped Thymio see its environment through Vision, while Alessandro guided him through the obscure and deadly areas using Navigation and Motion. Verter helped him stay on the right path all the way to the final portal using the magic filters.\n",
    "\n",
    "## Environment\n",
    "\n",
    "The world of Thymio, displayed below, is composed of\n",
    "- a light **background**, which is necessary to create a contrast with the black sheets, as we will use a threshold filter in the background. It is also required to guarantee good marker detection in `opencv-python`;\n",
    "- four aruco **markers** delimiting our map, allowing us to make a projection and therefore avoid any camera-related space distortion;\n",
    "- black **sheets** as global obstacles, even if we could have chosen any type and shape of object that is black;\n",
    "- two more **markers** for the robot and the target, from which we can easily take the position and angle thanks to `opencv-python`;\n",
    "- a light **bottle** or a can as a local obstacle, which must be high enough to be seen by the robot, and as light as possible to be invisible on the camera;\n",
    "\n",
    "![Setup](./assets/environment.jpeg)\n",
    "\n",
    "We think this setup covers all our needs with the least code necessary, because we are able to process all the data using the well-known `opencv-python` library, which already does a lot of work by itself."
   ],
   "id": "554798cd56e723e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:46:14.922928Z",
     "start_time": "2025-11-28T13:46:14.747778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.vision import *\n",
    "from src.consts import *\n",
    "from src.motion import *\n",
    "from src.utils import *\n",
    "from src.navigation import *"
   ],
   "id": "4389032ca8772d40",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vision\n",
    "\n",
    "The vision part focuses on getting all the available information on the real world, such as robot position, obstacles, and map delimitations. To do this, we divided the vision task into multiple smaller steps. Inside the codebase, it is visible on `vision.step()`.\n",
    "\n",
    "1. Connection to camera and image acquisition\n",
    "2. Aruco markers detection using `opencv-python`\n",
    "3. Projection using aruco markers position\n",
    "4. Target and robot position and orientation processing\n",
    "5. Final grid building, which will be used by other modules afterward\n",
    "\n",
    "We now describe each of these steps in a dedicated section in more depth. As the first step is quite straightforward, it will be skipped.\n",
    "\n",
    "### Markers detection\n",
    "\n",
    "One of the reasons we chose this setup is to be able to use `cv2.detectMarkers`, which will do a lot of work for us in this step.\n",
    "\n",
    "### Projection\n",
    "\n",
    "\n",
    "### Target and robot processing\n",
    "\n",
    "\n",
    "### Grid building\n",
    "\n"
   ],
   "id": "a81fb7e014d4974e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = start_vision()\n",
    "img = get_image(cap)\n",
    "\n",
    "plt.imshow(img[1])"
   ],
   "id": "2ac91e6b0bd64a96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then take a picture",
   "id": "fba9bd642a1f2c8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How to run\n",
    "\n",
    "For practical reasons, we did not do any run inside this notebook. You can, however, use the `main.py` file and run it to see the `cv2` window appear. Please note that you will need the six aruco markers to be able to make the robot move.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The final tests showed that our robot could reliably see its environment, avoid obstacles, and reach the portal as intended. This excellent outcome is the direct result of the non-stop, shared effort of our group over the project's few weeks. Our collective work confirmed that our combined skills were perfectly suited to solve this challenge.\n",
    "\n",
    "We wish to thank Professor Mondada and all the assistants for their support and guidance throughout the project.\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "- Aruco markers detection: https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html\n",
    "- OpenCV GUI: https://docs.opencv.org/4.x/d7/dfc/group__highgui.html"
   ],
   "id": "3ded4d48cedd368a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
